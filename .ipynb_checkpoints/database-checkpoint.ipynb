{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from retrying import retry\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import math\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow,Flow\n",
    "from google.auth.transport.requests import Request\n",
    "import os\n",
    "import pickle\n",
    "import ulta_functions as ulta\n",
    "import google_api_functions as gapi\n",
    "import google_sheets_credentials as creds\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import copy\n",
    "import concurrent.futures\n",
    "import json\n",
    "import datetime\n",
    "import psycopg2\n",
    "from psycopg2.errors import UniqueViolation\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the ids to create real urls\n",
    "def create_url_df(session):\n",
    "    all_url_info = {}\n",
    "    #I'm pulling the list of urls straight from ulta's sidebar\n",
    "    front_page = session.get('https://www.ulta.com/')\n",
    "    front_page_soup = BeautifulSoup(front_page.text, features=\"lxml\")\n",
    "    #anchors = list of links in the side bar\n",
    "    anchors = front_page_soup.find_all('a', {'class' : 'Anchor'})\n",
    "    for anchor in anchors:\n",
    "        #make sure there's a description; I'm getting the categories from the description\n",
    "        if anchor.get('data-nav-description') is not None and re.search(r'[a-z]*:[a-z]*', anchor.get('data-nav-description')) is not None:\n",
    "            #split up url path into pieces\n",
    "            url_path = anchor.get('data-nav-description')[4:].split(':')\n",
    "            #I do not want urls from these anchors\n",
    "            if url_path[0] not in ['shop by brand', 'new arrivals', 'ulta beauty collection', 'gifts', 'sale & coupons', 'beauty tips'] and url_path[1] != 'featured':\n",
    "                page = session.get(anchor.get('href'))\n",
    "                soup = BeautifulSoup(page.text, features=\"lxml\")\n",
    "                #get the number of total products from each id so we can create a different url for each set of 500 products in the url so there isn't too much data loaded into one url at once\n",
    "                num_results = int(re.findall(r'\\b\\d+\\b', soup.find('h2', {'class' : 'search-res-title'}).find('span', {'class' : 'sr-only'}).text)[0])\n",
    "                for i in range(math.ceil(num_results / 500)):\n",
    "                    #creating a dictionary to have each url be linked to its id, main category, and sub category\n",
    "                    url_info = {}\n",
    "                    url_info['main_category'] = url_path[0]\n",
    "                    url_info['sub_category'] = url_path[1]\n",
    "                    if len(url_path) == 2: #if the length != 2 then the url path has at least 3 parts which means we can get a sub sub sub category from it \n",
    "                        url_info['sub_sub_category'] = ' '\n",
    "                    else:\n",
    "                        url_info['sub_sub_category'] = url_path[2]\n",
    "                    #the &No= tag is the number of products on that page starting from 0 and &Nrpp=500 means there will be at most 500 products on each page\n",
    "                    url = anchor.get('href') + '&No=' + str(i * 500) + '&Nrpp=500'\n",
    "                    all_url_info[url] = url_info\n",
    "    url_df = (\n",
    "        pd.DataFrame.from_dict(all_url_info)\n",
    "        .transpose()\n",
    "        .reset_index()\n",
    "        .rename(columns={'index' : 'url'})\n",
    "        .rename_axis('url_pkey')\n",
    "    )\n",
    "    url_df.to_csv('data/url_df.csv')\n",
    "    \n",
    "def get_url_df(session):\n",
    "    #getting the last modified date of my url_df.csv file\n",
    "    last_mod_time = os.path.getmtime('data/url_df.csv')\n",
    "    #getting number of days since last file modification date\n",
    "    days_since_urls_update = (datetime.datetime.today() - datetime.datetime.fromtimestamp(last_mod_time)).days\n",
    "    #if it has been at least 5 days since the last time the all_url_info_dict.json file was modified, then update\n",
    "    if days_since_urls_update >= 5:\n",
    "        create_url_df(session)\n",
    "    #return url_df\n",
    "    url_df = pd.read_csv('data/url_df.csv')\n",
    "    return(url_df)\n",
    "\n",
    "def scrape_url(session, products, row):\n",
    "    #going to the url\n",
    "    page = session.get(row['url'])\n",
    "    #getting the page's content and using the package BeautifulSoup to extract data from it\n",
    "    soup = BeautifulSoup(page.text, features=\"lxml\")\n",
    "    #each product on ulta's website has a container with the class \"productQvContainer\" so I'm getting every element that has that as a class to pull every product\n",
    "    product_containers = soup.find_all('div', {'class' : 'productQvContainer'})\n",
    "    #applying the function get_single_product for each product in the url. if it throws an exception, I'm having it print the url and index so I can tell what product is having a problem.\n",
    "    for product_container in product_containers:\n",
    "        try:\n",
    "            product, product_id = get_single_product(soup, product_container, row.name)\n",
    "            products[product_id] = product\n",
    "        except Exception as exc:\n",
    "            print(row['url'], product_containers.index(product_container))\n",
    "            print(exc, '\\n')\n",
    "    return(products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_product(soup, product_container, url_pkey):\n",
    "    product = {}\n",
    "    #get general product data from each product\n",
    "    product_id = product_container.find('span', {'class' : 'prod-id'}).text.strip()\n",
    "    product['sku_id'] = str(product_container.find('a', {'class' : 'qShopbutton'}).get('data-skuidrr'))\n",
    "    product['brand'] = product_container.find('h4', {'class' : 'prod-title'}).text.strip()\n",
    "    #description is the name of the product. so if there's a product called \"ULTA Fabulous Concealer\", \"ULTA\" would be the brand and \"Fabulous Concealer\" would be the description.\n",
    "    product['product'] = product_container.find('p', {'class' : 'prod-desc'}).text.strip()\n",
    "    #sometimes the https://www.ulta.com is already in the url and sometimes (most of the time) it's not.\n",
    "    if product_container.find('a', {'class' : 'product'}).get('href')[0] != '/':\n",
    "        product_url = product_container.find('a', {'class' : 'product'}).get('href')\n",
    "    else:\n",
    "        product_url = 'https://www.ulta.com' + product_container.find('a', {'class' : 'product'}).get('href')\n",
    "    #if the correct product id isn't in the url then the url is wrong. if it's wrong, then we need to fix it.\n",
    "    if product_url.split('productId=')[1] != product_id:\n",
    "        product_url = 'https://www.ulta.com/' + product['product'].replace(' ', '-').lower() + '?productId=' + product_id\n",
    "    product['url'] = product_url\n",
    "    #getting the rating information for each product; using if statements in case a product doesn't have a rating for whatever reason\n",
    "    if product_container.find('label', {'class' : 'sr-only'}) is not None:\n",
    "        rating = product_container.find('label', {'class' : 'sr-only'}).text.split(' ')[0]\n",
    "        if rating == 'Price':\n",
    "            rating = 0.00\n",
    "        product['rating'] = rating\n",
    "    if product_container.find('span', {'class' : 'prodCellReview'}) is not None:\n",
    "        product['no_of_reviews'] = re.findall(r'\\b\\d+\\b', product_container.find('span', {'class' : 'prodCellReview'}).text)[0]\n",
    "    #the prices are labeled differently in the code depending on whether the product is for sale or not (for sale as in marked as sale not a secret sale)\n",
    "    if product_container.find('div', {'class' : 'productSale'}) is None:\n",
    "        product['sale'] = 0\n",
    "        product['price'] = product_container.find('span', {'class' : 'regPrice'}).text.strip()\n",
    "    else:\n",
    "        product['sale'] = 1\n",
    "        product['price'] = product_container.find('span', {'class' : 'pro-old-price'}).text.strip()\n",
    "        product['sale_price'] = product_container.find('span', {'class' : 'pro-new-price'}).text.strip()\n",
    "    #getting the available offers and number of options/colors of the product if they're listed\n",
    "    if product_container.find('div', {'class' : 'product-detail-offers'}) is not None:\n",
    "        product['offers'] = product_container.find('div', {'class' : 'product-detail-offers'}).text.strip()\n",
    "    if product_container.find('span', {'class' : 'pcViewMore'}) is not None:\n",
    "        product['options'] = re.sub('\\xa0', ' ', product_container.find('span', {'class' : 'pcViewMore'}).text.strip())\n",
    "    product['url_pkey_foreign'] = url_pkey\n",
    "    return(product, product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_in_stock(product_id, url):\n",
    "    #start = time.perf_counter()\n",
    "    #chrome_options = Options()\n",
    "    #chrome_options.add_argument(\"--headless\")\n",
    "    #with webdriver.Chrome(r'C:\\Users\\elerm\\Downloads\\chromedriver_win32\\chromedriver.exe', options = chrome_options) as driver:\n",
    "    with webdriver.Chrome(r'C:\\Users\\elerm\\Downloads\\chromedriver_win32\\chromedriver.exe') as driver:\n",
    "        product_in_stock = {}\n",
    "        variants_in_stock = {}\n",
    "        wait = WebDriverWait(driver, 60)\n",
    "        #opening product url in the driver/browser\n",
    "        driver.get(url)\n",
    "        #if the product doesn't exist anymore ulta wil take you to this site\n",
    "        if driver.current_url == 'https://www.ulta.com/404.jsp':\n",
    "            next\n",
    "        wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'ProductSwatchImage__variantHolder')))\n",
    "        element = driver.find_element_by_class_name('Reviews__container--list')\n",
    "        wait.until(EC.visibility_of(element))\n",
    "        product_variants = driver.find_elements_by_class_name('ProductSwatchImage__variantHolder')\n",
    "        if len(product_variants) == 0:\n",
    "            #products that only have one color or one size or whatever have their product variant information in a different lcoation\n",
    "            product_variants = driver.find_elements_by_class_name('ProductDetail__productSwatches')\n",
    "        print(len(product_variants))\n",
    "        #getting all the product variants from the page\n",
    "        #product_variants = get_product_variants(driver)\n",
    "        for product_variant in product_variants:\n",
    "            element_pic = driver.find_element_by_class_name(\"ProductDetail__productImage\")\n",
    "            try:\n",
    "                product_variant.click() #clicking on each variant at a time to get their price and availability\n",
    "            except:         \n",
    "                next #if I can't click on it I want to go to the next variant\n",
    "            else:\n",
    "                wait.until(EC.visibility_of(element_pic))\n",
    "                #creating a BeautifulSoup object to extract data\n",
    "                soup = BeautifulSoup(driver.page_source, features=\"lxml\")\n",
    "                #there are products that only a couple of shades are labeled as sale so I'm removing those to make sure no sale items slip through\n",
    "                if soup.find('img', {'src' : 'https://images.ulta.com/is/image/Ulta/badge-sale?fmt=png-alpha'}) is not None:\n",
    "                    next\n",
    "                #getting price\n",
    "                price = soup.find('meta', {'property' : 'product:price:amount'}).get('content')\n",
    "                option = ulta.get_option(soup)\n",
    "                #only adding the product variant if it's available\n",
    "                if soup.find('div', {'class' : 'ProductDetail__availabilitySection ProductDetail__availabilitySection--error'}) is None:\n",
    "                    variants_in_stock[option] = price\n",
    "        product_in_stock[product_id] = variants_in_stock\n",
    "    finish = time.perf_counter()\n",
    "    print(len(product_in_stock))\n",
    "    print(f'{product_id} Finished in {round(finish-start, 2)} second(s)')\n",
    "    return(product_in_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product variant page doesn't finish loading and variant information is not extracted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_variants(driver):\n",
    "    product_variants1 = 0\n",
    "    product_variants2 = -1\n",
    "    while(product_variants1 != product_variants2):\n",
    "        product_variants1 = driver.find_elements_by_class_name('ProductSwatchImage__variantHolder')\n",
    "        if len(product_variants1) == 0:\n",
    "            #products that only have one color or one size or whatever have their product variant information in a different lcoation\n",
    "            product_variants1 = driver.find_elements_by_class_name('ProductDetail__productSwatches')    \n",
    "        product_variants2 = driver.find_elements_by_class_name('ProductSwatchImage__variantHolder')\n",
    "        if len(product_variants2) == 0:\n",
    "            #products that only have one color or one size or whatever have their product variant information in a different lcoation\n",
    "            product_variants2 = driver.find_elements_by_class_name('ProductDetail__productSwatches')\n",
    "            print(len(product_variants1), len(product_variants2))\n",
    "    return(product_variants1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "products_t = dict(itertools.islice(products.items(), 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "xlsImpprod5770263\n",
      "44\n",
      "xlsImpprod15711051\n",
      "xlsImpprod14491009\n",
      "2\n",
      "xlsImpprod3590053\n",
      "5\n",
      "xlsImpprod10791925\n",
      "0\n",
      "1\n",
      "xlsImpprod12011171 Finished in 9.88 second(s)\n",
      "{'xlsImpprod12011171': {}} \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in products_t.keys():\n",
    "    try:\n",
    "        result = get_product_in_stock(key, products[key]['url'])\n",
    "        print(result, '\\n')\n",
    "    except:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "1\n",
      "xlsImpprod10791925 Finished in 146620.03 second(s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'xlsImpprod10791925': {'Calming Fair (very fair, cool-neutral undertones)': '31.00',\n",
       "  'Calming Ivory (very fair, cool-neutral undertones)': '31.00'}}"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_product_in_stock('xlsImpprod10791925', 'https://www.ulta.com/redness-solutions-makeup-broad-spectrum-spf-15-with-probiotic-technology?productId=xlsImpprod10791925')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url2(row):\n",
    "    products = {}\n",
    "    #going to the url\n",
    "    page = requests.get(row['url'])\n",
    "    #getting the page's content and using the package BeautifulSoup to extract data from it\n",
    "    soup = BeautifulSoup(page.text, features=\"lxml\")\n",
    "    #each product on ulta's website has a container with the class \"productQvContainer\" so I'm getting every element that has that as a class to pull every product\n",
    "    product_containers = soup.find_all('div', {'class' : 'productQvContainer'})\n",
    "    #applying the function get_single_product for each product in the url. if it throws an exception, I'm having it print the url and index so I can tell what product is having a problem.\n",
    "    for product_container in product_containers:\n",
    "        try:\n",
    "            product, product_id = get_single_product(soup, product_container, row.name)\n",
    "            products[product_id] = product\n",
    "        except Exception as exc:\n",
    "            print(row['url'], product_containers.index(product_container))\n",
    "            print(exc, '\\n')\n",
    "    products_df = (\n",
    "        pd.DataFrame.from_dict(products)\n",
    "        .transpose()\n",
    "    )\n",
    "    return(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_soup(row):\n",
    "    products = {}\n",
    "    #going to the url\n",
    "    page = requests.get(row['url'])\n",
    "    #getting the page's content and using the package BeautifulSoup to extract data from it\n",
    "    soup = BeautifulSoup(page.text, features=\"lxml\")\n",
    "    time.sleep(10)\n",
    "    return(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 1: scrape current data from ulta.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "url_df = get_url_df(session).set_index('url_pkey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_df_t = url_df[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for index, row in url_df_t.iterrows():\n",
    "    rows.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for index, row in url_df_t.iterrows():\n",
    "    t = threading.Thread(target=get_soup, args=[row])\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18887959999847226\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    results = executor.map(get_soup, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.551527000003261\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for index, row in url_df_t.iterrows():\n",
    "    p = multiprocessing.Process(target=get_soup, args=[row])\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1754369999980554\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(get_soup, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6237601999891922\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_something(seconds):\n",
    "    print('sleeping...')\n",
    "    time.sleep(seconds)\n",
    "    return(seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping...\n",
      "sleeping...sleeping...\n",
      "\n",
      "sleeping...\n",
      "sleeping...\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "for i in [5, 4, 3, 2, 1]:\n",
    "    t = threading.Thread(target=do_something, args=[i])\n",
    "    t.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05658979999134317\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sleeping...\n",
      "sleeping...\n",
      "sleeping...\n",
      "sleeping...\n",
      "sleeping...\n"
     ]
    }
   ],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    secs = [5, 4, 3, 2, 1]\n",
    "    results = executor.map(do_something, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.0343901999876834\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "for i in [5, 4, 3, 2, 1]:\n",
    "    p = multiprocessing.Process(target=do_something, args=[i])\n",
    "    p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19040530000347644\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.perf_counter()\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    results = executor.map(do_something, secs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.778286200016737\n"
     ]
    }
   ],
   "source": [
    "end = time.perf_counter()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ulta_df = (\n",
    "    pd.DataFrame.from_dict(products)\n",
    "    .transpose()\n",
    "    .rename_axis('product_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ulta_df.to_csv('data/current_ulta_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dealing with sale prices\n",
    "\n",
    "if there's a sale price, that's the price the product is currently being sold for, so I want the price of the product in the database to be the sale price. so if there's a sale price I'm replacing the price with the sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_price(row):\n",
    "    if row['sale'] == 0:\n",
    "        val = row['price']\n",
    "    else:\n",
    "        val = row['sale_price']\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ulta_df['price_str'] = current_ulta_df.apply(fix_price, axis=1)\n",
    "current_ulta_df['date'] = [datetime.date.today().strftime('%Y-%m-%d')] * len(current_ulta_df)\n",
    "current_ulta_df = (\n",
    "    current_ulta_df\n",
    "    .drop(columns={'sale_price'})\n",
    "    .fillna(value={'options' : ''})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_ulta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: find secret sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### connect to database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = config.config()\n",
    "conn = psycopg2.connect(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    SELECT DISTINCT\n",
    "        product.product_id, tbl.max_date, price.price_str, price.sale\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_pkey_foreign, MAX(price_entry_date) as max_date\n",
    "        FROM \n",
    "            price\n",
    "        GROUP BY \n",
    "            product_pkey_foreign\n",
    "        ) tbl\n",
    "    LEFT JOIN \n",
    "        product\n",
    "    ON\n",
    "        tbl.product_pkey_foreign = product.product_pkey\n",
    "    INNER JOIN\n",
    "        price\n",
    "    ON\n",
    "        tbl.product_pkey_foreign = price.product_pkey_foreign and tbl.max_date = price.price_entry_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = cur.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dat = pd.DataFrame(r, columns=['product_id', 'recent_date', 'recent_price_str', 'recent_sale'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat = (\n",
    "    current_ulta_df\n",
    "    .pipe(pd.merge, db_dat, on='product_id', how='left')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    merged_dat\n",
    "    .query(\"price != price_str\")\n",
    "    .loc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dat = pd.DataFrame(r, columns=['product_id', 'rating', 'no_of_reviews', 'offers', 'max_date', 'price_str', 'max_price', 'option'])\n",
    "\n",
    "latest_ulta_dat = (\n",
    "    db_dat\n",
    "    .drop(columns={'max_price', 'option'})\n",
    "    .pipe(pd.DataFrame.drop_duplicates)\n",
    "    .sort_values(['product_id', 'max_date'])\n",
    "    .pipe(pd.DataFrame.drop_duplicates, 'product_id', keep='last')\n",
    "    .set_index('product_id')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_ulta_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat = (\n",
    "    current_ulta_df\n",
    "    .pipe(pd.merge, latest_ulta_dat, on='product_id', how='left')\n",
    ")\n",
    "\n",
    "new = []\n",
    "for i in range(len(merged_dat)):\n",
    "    if merged_dat.iloc[i]['price_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dat.iloc[16879]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_dat.query(\"product_id == 'pimprod2006617'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_db = (\n",
    "    current_ulta_df\n",
    "    .pipe(pd.merge, db_dat, on='product_id', how='left')\n",
    "    .dropna(subset=['price_str'])\n",
    "    .query('price != price_str | rating_x != rating_y')\n",
    "    .drop(columns={'rating_y', 'no_of_reviews_y', 'offers_y', 'max_date', 'price_str'})\n",
    "    .rename(columns={'rating_x' : 'rating', 'no_of_reviews_x' : 'no_of_reviews', 'offers_x' : 'offers'})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you make a mistake, execute following code afterwards\n",
    "#cur.execute(\"rollback;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
